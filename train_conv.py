# -*- coding: utf-8 -*-
"""train_conv.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14RQ7ixGwL4PwwH4nK8VEGAoQtjSGXWQf
"""

import numpy as np
import matplotlib.pyplot as plt
import os
import pywt
import zipfile
import fnmatch

import tensorflow as tf
from sklearn.preprocessing import StandardScaler

tf.random.set_seed(1234)
np.random.seed(seed=1234)
os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'
tf.config.list_physical_devices('GPU') 
!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')
!ls '/content/drive/My Drive/Google Colab/COMP562FinalProject'

feats_to_use = [1, 3, 7, -1]

fs = 100

def data_from_zip(path):
    data = []
    with zipfile.ZipFile(path) as zf:
        file_list = zf.namelist()
        files = fnmatch.filter(file_list, '*.npy')

        for f in files:
            d = np.load(zf.open(f))[1000:,feats_to_use]

            if data == []:
                data = d
            else:
                data = np.vstack((data,d))

    return data

train_path = '/content/drive/My Drive/Google Colab/COMP562FinalProject/data/augment.zip'
test_path = '/content/drive/My Drive/Google Colab/COMP562FinalProject/data/test.zip'

train_val_data = data_from_zip(train_path)
test_data = data_from_zip(test_path)

n = train_val_data.shape[0]

train_data = train_val_data[:int(n*0.75),:]
val_data = train_val_data[int(n*0.75):,:]

train_mean = train_data[:,:-1].mean()
train_std = train_data[:,:-1].std()

train_data[:,:-1] = (train_data[:,:-1] - train_mean) / train_std
val_data[:,:-1] = (val_data[:,:-1] - train_mean) / train_std
test_data[:,:-1] = (test_data[:,:-1] - train_mean) / train_std


class WindowGenerator():
    def __init__(self, input_width, label_width, shift, strides,
               label_columns_indices,
               train=train_data, val=val_data, test=test_data):

        self.train_data = train
        self.val_data = val
        self.test_data = test

        self.label_columns_indices = label_columns_indices

        self.input_width = input_width
        self.label_width = label_width
        self.shift = shift
        self.strides = strides

        self.total_window_size = input_width + shift

        self.input_slice = slice(0, input_width)
        self.input_indices = np.arange(self.total_window_size)[self.input_slice]

        self.label_start = self.total_window_size - self.label_width
        self.labels_slice = slice(self.label_start, None)
        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]

    def __repr__(self):
        return '\n'.join([
            f'Total window size: {self.total_window_size}',
            f'Input indices: {self.input_indices}',
            f'Label indices: {self.label_indices}',
            f'Label column indice(s): {self.label_columns_indices}'])

    def split_window(self, features):
        inputs = features[:, self.input_slice, :]
        labels = features[:, self.labels_slice, :]

        if self.label_columns_indices is not None:
            inputs = tf.stack(
                [inputs[:, :, k] for k in np.delete(
                        np.arange(self.train_data.shape[-1]), 
                        self.label_columns_indices)],
                axis=-1)

            labels = tf.stack(
                [labels[:, :, k] for k in self.label_columns_indices],
                axis=-1)

        # Slicing doesn't preserve static shape information, so set the shapes
        # manually. This way the `tf.data.Datasets` are easier to inspect.
        inputs.set_shape([None, self.input_width, None])
        labels.set_shape([None, self.label_width, None])
        print('shapes:')
        print(inputs.shape)
        print(labels.shape)
        return inputs, labels

    def make_dataset(self, data):
        ds = tf.keras.preprocessing.timeseries_dataset_from_array(
            data=data,
            targets=None,
            sequence_length=self.total_window_size,
            sequence_stride=self.strides,
            shuffle=True,
            batch_size=128,)

        ds = ds.map(self.split_window)
        return ds

    @property
    def train(self):
        return self.make_dataset(self.train_data)

    @property
    def val(self):
        return self.make_dataset(self.val_data)

    @property
    def test(self):
        return self.make_dataset(self.test_data)

    @property
    def example(self):
        result = getattr(self, '_example', None)
        if result is None:
            # No example batch was found, so get one from the `.val` dataset
            result = next(iter(self.val))
            # And cache it for next time
            self._example = result
        return result

    def plot(self, model=None, plot_col_index=-1, max_subplots=30):
        inputs, labels = self.example
        plt.figure(figsize=(12, 80))
        max_n = min(max_subplots, len(inputs))
        for n in range(max_n):
            plt.subplot(max_n, 1, n+1)
            plt.ylabel('Slope')
            #plt.plot(self.input_indices, inputs[n, :, plot_col_index],
             #       label='Inputs', marker='.', zorder=-10)
            plt.scatter(self.label_indices, labels[n, :, plot_col_index],
                        label='Labels', c='#2ca02c', s=64)
            if model is not None:
                predictions = model(inputs)
                plt.scatter(self.label_indices, predictions[n, :, plot_col_index],
                            marker='X', label='Predictions',
                            c='#ff7f0e', s=64)

            if n == 0:
                plt.legend()

        plt.xlabel('Samples')
        plt.show()


def compile_and_fit(model, window, patience=3):
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                    patience=patience,
                                                    mode='min',
                                                    restore_best_weights=True)

    model.compile(loss=tf.losses.MeanSquaredError(),
                optimizer= tf.train.experimental.enable_mixed_precision_graph_rewrite(tf.optimizers.Adam(learning_rate=0.001)),
                metrics=[tf.metrics.RootMeanSquaredError()])

    history = model.fit(window.train, epochs=MAX_EPOCHS,
                      validation_data=window.val,
                      callbacks=[early_stopping])
    return history

class ResidualBlock(tf.keras.layers.Layer):
    def __init__(self, input_depth, kernel_size, inner_depth, new_stage=False):
        super().__init__()
        self.new_stage = new_stage
        if (self.new_stage):
            init_strides = 1
            self.shortcut_pad = tf.keras.Sequential([
                tf.keras.layers.Conv1D(filters=input_depth*2, kernel_size=(1,))
                ])
        else:
            init_strides = 1

        self.block_1 = tf.keras.Sequential([
            tf.keras.layers.Conv1D(filters=inner_depth, kernel_size=(1,), strides=init_strides),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.LeakyReLU(),

            tf.keras.layers.Conv1D(filters=inner_depth, kernel_size=(kernel_size,), padding='same'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.LeakyReLU(),

            tf.keras.layers.Conv1D(filters=(input_depth/2 if not new_stage else input_depth), kernel_size=(1,)),
            tf.keras.layers.BatchNormalization(),                         
        ])

        self.block_2 = tf.keras.Sequential([
            tf.keras.layers.Conv1D(filters=inner_depth, kernel_size=(1,), strides=init_strides),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.LeakyReLU(),

            tf.keras.layers.Conv1D(filters=inner_depth, kernel_size=(kernel_size*2,), padding='same'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.LeakyReLU(),

            tf.keras.layers.Conv1D(filters=(input_depth/2 if not new_stage else input_depth), kernel_size=(1,)),
            tf.keras.layers.BatchNormalization(),                         
        ])

    def call(self, input, *args, **kwargs):
        if (self.new_stage):
            return tf.nn.leaky_relu(tf.concat([self.block_1(input),self.block_2(input)], -1) + self.shortcut_pad(input))
        else:
            return tf.nn.leaky_relu(tf.concat([self.block_1(input),self.block_2(input)], -1) + input)


conv_model_300_1 = tf.keras.Sequential([
    tf.keras.layers.Conv1D(filters=128,
                           kernel_size=(136,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.LeakyReLU(alpha=0.01),
    tf.keras.layers.MaxPool1D(6, strides=1, padding='valid'),
    
    tf.keras.layers.Conv1D(filters=128,
                           kernel_size=(128,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.LeakyReLU(alpha=0.01),
    tf.keras.layers.MaxPool1D(3, strides=1, padding='valid'),
        
    tf.keras.layers.Conv1D(filters=512,
                           kernel_size=(31,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.LeakyReLU(alpha=0.01),

    tf.keras.layers.Dense(units=512, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(units=1),
])

conv_model_500_1 = tf.keras.Sequential([
    tf.keras.layers.Conv1D(filters=128,
                           kernel_size=(336,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.LeakyReLU(alpha=0.01),
    tf.keras.layers.MaxPool1D(6, strides=1, padding='valid'),
    
    tf.keras.layers.Conv1D(filters=128,
                           kernel_size=(128,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.LeakyReLU(alpha=0.01),
    tf.keras.layers.MaxPool1D(3, strides=1, padding='valid'),
        
    tf.keras.layers.Conv1D(filters=512,
                           kernel_size=(31,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.LeakyReLU(alpha=0.01),

    tf.keras.layers.Dense(units=512, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(units=1),
])

conv_model_500_res  = tf.keras.Sequential([
    tf.keras.layers.Conv1D(filters=128,strides=1, kernel_size=(376,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.LeakyReLU(),
    tf.keras.layers.MaxPool1D(35, strides=1),

    ResidualBlock(128, 32, 64),
    ResidualBlock(128, 32, 64),

    tf.keras.layers.MaxPool1D(31, strides=1),

    ResidualBlock(128, 16, 128, True),
    ResidualBlock(256, 16, 128),

    tf.keras.layers.MaxPool1D(31, strides=1),

    ResidualBlock(256, 8, 256, True),
    ResidualBlock(512, 8, 256),

    tf.keras.layers.AveragePooling1D(31, strides=1),

    tf.keras.layers.Dens 64e(units=512, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(units=1),
])

conv_model_700_1 = tf.keras.Sequential([
    tf.keras.layers.Conv1D(filters=128,
                           kernel_size=(536,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.LeakyReLU(alpha=0.01),
    tf.keras.layers.MaxPool1D(6, strides=1, padding='valid'),
    
    tf.keras.layers.Conv1D(filters=128,
                           kernel_size=(128,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.LeakyReLU(alpha=0.01),
    tf.keras.layers.MaxPool1D(3, strides=1, padding='valid'),
        
    tf.keras.layers.Conv1D(filters=512,
                           kernel_size=(31,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.LeakyReLU(alpha=0.01),

    tf.keras.layers.Dense(units=512, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(units=1),
])

conv_model_900_1 = tf.keras.Sequential([
    tf.keras.layers.Conv1D(filters=128,
                           kernel_size=(736,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.LeakyReLU(alpha=0.01),
    tf.keras.layers.MaxPool1D(6, strides=1, padding='valid'),
    
    tf.keras.layers.Conv1D(filters=128,
                           kernel_size=(128,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.LeakyReLU(alpha=0.01),
    tf.keras.layers.MaxPool1D(3, strides=1, padding='valid'),
        
    tf.keras.layers.Conv1D(filters=512,
                           kernel_size=(31,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.LeakyReLU(alpha=0.01),

    tf.keras.layers.Dense(units=512, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(units=1),
])

models = [conv_model_500_res]

MAX_EPOCHS = 20
eval_width = 1000
window_widths = fs * np.array([5])
val_performance = {}
performance = {}

for i, window_width in enumerate(window_widths):
    model = models[i]
    conv_width = window_width

    conv_window = WindowGenerator(
        input_width=conv_width,
        label_width=1,
        shift=0,
        strides=2,
        label_columns_indices=[-1])

    wide_eval_window = WindowGenerator(
        input_width=eval_width, 
        label_width=eval_width, 
        shift=0, strides=1, 
        label_columns_indices=[-1])

    wide_conv_window = WindowGenerator(
        input_width=(conv_width-1)+eval_width,
        label_width=eval_width,
        shift=0,
        strides=1,
        label_columns_indices=[-1])


    print("Wide conv window")
    print('Input shape:', wide_conv_window.example[0].shape)
    print('Labels shape:', wide_conv_window.example[1].shape)
    print('Output shape:', model(wide_conv_window.example[0]).shape)

    history = compile_and_fit(model, conv_window)
    model.summary()

    wide_conv_window.plot(model)
    val_performance['Conv'] = model.evaluate(conv_window.val)
    performance['Conv'] = model.evaluate(conv_window.test, verbose=0)

eval_width = 2000
wide_conv_window = WindowGenerator(
    input_width=(conv_width-1)+eval_width,
    label_width=eval_width,
    shift=0,
    strides=1,
    label_columns_indices=[-1])
wide_conv_window.plot(model)

plt.plot(test_data[:1000,2])

plt.plot(train_data[-1000:,2])

