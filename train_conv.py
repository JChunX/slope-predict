# -*- coding: utf-8 -*-
"""train_conv.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14RQ7ixGwL4PwwH4nK8VEGAoQtjSGXWQf
"""

import numpy as np
import matplotlib.pyplot as plt
import os
import pywt
import zipfile
import fnmatch
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib
font = {'family' : 'normal',
        'weight' : 'bold',
        'size'   : 22}

matplotlib.rc('font', **font)

import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
import seaborn as sns

tf.random.set_seed(1234)
np.random.seed(seed=1234)

from google.colab import drive
drive.mount('/content/drive')
!ls '/content/drive/My Drive/Google Colab/COMP562FinalProject'
!pip install pyyaml h5py  # Required to save models in HDF5 format

from scipy.signal import resample
feats_to_use = [1, 3, 7, -1]

def data_from_zip(path):
    data = []
    with zipfile.ZipFile(path) as zf:
        file_list = zf.namelist()
        files = fnmatch.filter(file_list, '*.npy')

        for f in files:
            d = np.load(zf.open(f))[1000:,feats_to_use]
            if data == []:
                data = d

            else:
                data = np.vstack((data,d))
    return data

train_path = '/content/drive/My Drive/Google Colab/COMP562FinalProject/data/augment.zip'
test_path = '/content/drive/My Drive/Google Colab/COMP562FinalProject/data/test.zip'

train_val_data = data_from_zip(train_path)
test_data = data_from_zip(test_path)
test_data = resample(test_data, int(len(test_data)/3))

n = train_val_data.shape[0]

train_data = train_val_data[:int(n*0.75),:]
val_data = train_val_data[int(n*0.75):,:]

train_mean = np.mean(train_data[:,:-1], axis=0)
train_std = np.std(train_data[:,:-1], axis=0)

train_data[:,:-1] = np.divide(np.subtract(train_data[:,:-1], train_mean), train_std)
val_data[:,:-1] = np.divide(np.subtract(val_data[:,:-1], train_mean), train_std)
test_data[:,:-1] = np.divide(np.subtract(test_data[:,:-1], train_mean), train_std)

class WindowGenerator():
    def __init__(self, input_width, label_width, shift, strides,
               label_columns_indices,
               train=train_data, val=val_data, test=test_data):

        self.train_data = train
        self.val_data = val
        self.test_data = test

        self.label_columns_indices = label_columns_indices

        self.input_width = input_width
        self.label_width = label_width
        self.shift = shift
        self.strides = strides

        self.total_window_size = input_width + shift

        self.input_slice = slice(0, input_width)
        self.input_indices = np.arange(self.total_window_size)[self.input_slice]

        self.label_start = self.total_window_size - self.label_width
        self.labels_slice = slice(self.label_start, None)
        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]

    def __repr__(self):
        return '\n'.join([
            f'Total window size: {self.total_window_size}',
            f'Input indices: {self.input_indices}',
            f'Label indices: {self.label_indices}',
            f'Label column indice(s): {self.label_columns_indices}'])

    def split_window(self, features):
        inputs = features[:, self.input_slice, :]
        labels = features[:, self.labels_slice, :]

        if self.label_columns_indices is not None:
            inputs = tf.stack(
                [inputs[:, :, k] for k in np.delete(
                        np.arange(self.train_data.shape[-1]), 
                        self.label_columns_indices)],
                axis=-1)

            labels = tf.stack(
                [labels[:, :, k] for k in self.label_columns_indices],
                axis=-1)

        # Slicing doesn't preserve static shape information, so set the shapes
        # manually. This way the `tf.data.Datasets` are easier to inspect.
        inputs.set_shape([None, self.input_width, None])
        labels.set_shape([None, self.label_width, None])
        print('shapes:')
        print(inputs.shape)
        print(labels.shape)
        return inputs, labels

    def make_dataset(self, data):
        ds = tf.keras.preprocessing.timeseries_dataset_from_array(
            data=data,
            targets=None,
            sequence_length=self.total_window_size,
            sequence_stride=self.strides,
            shuffle=True,
            batch_size=48,)

        ds = ds.map(self.split_window)
        return ds

    @property
    def train(self):
        return self.make_dataset(self.train_data)

    @property
    def val(self):
        return self.make_dataset(self.val_data)

    @property
    def test(self):
        return self.make_dataset(self.test_data)

    @property
    def example(self):
        result = getattr(self, '_example', None)
        if result is None:
            # No example batch was found, so get one from the `.val` dataset
            result = next(iter(self.test))
            # And cache it for next time
            self._example = result
        return result

    def plot(self, model=None, plot_col_index=-1, max_subplots=30, conf_mat=False):
        inputs, labels = self.example
        plt.figure(figsize=(15, 12*max_subplots))
        max_n = min(max_subplots, len(inputs))
        for n in range(max_n):
            plt.subplot(max_n, 1, n+1)
            plt.ylabel('Slope (Degrees)')
            #plt.plot(self.input_indices, inputs[n, :, plot_col_index],
             #       label='Inputs', marker='.', zorder=-10)
            plt.scatter(self.label_indices, labels[n, :, plot_col_index],
                        label='Labels', c='#2ca02c', s=64)
            if model is not None:
                predictions = model(inputs)
                plt.scatter(self.label_indices, predictions[n, :, plot_col_index],
                            marker='X', label='Predictions',
                            c='#ff7f0e', s=64)

            if n == 0:
                plt.legend()

            if (conf_mat):
                plt.title('CNN Predictions vs. Labels')
                plt.xlabel('Samples')
                plt.show()

                label_conf = labels[n, :, -1]
                pred_conf = predictions[n, :, -1]

                bins = [0,1,2,3,4,5,6,7]
                label_conf = np.digitize(label_conf,bins).astype(int)
                pred_conf = np.digitize(pred_conf,bins).astype(int)
                print(label_conf)
                print(pred_conf)
                plt.figure(figsize=(20,20))
                sns.heatmap(confusion_matrix(label_conf, pred_conf, normalize='true'), annot=True)
                plt.ylabel('Label')
                plt.xlabel('Prediction')
                plt.title('CNN Binned Confusion Matrix')

fs = 33

def compile_and_fit(model, window, patience=3):
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                    patience=patience,
                                                    mode='min',
                                                    restore_best_weights=True)

    model.compile(loss=tf.losses.MeanAbsoluteError(),
                optimizer=tf.optimizers.Adam(learning_rate=0.00005),
                metrics=[tf.metrics.RootMeanSquaredError()])

    history = model.fit(window.train, epochs=MAX_EPOCHS,
                    validation_data=window.val,
                    callbacks=[early_stopping])
    return history

class ResidualBlock(tf.keras.layers.Layer):
    def __init__(self, input_depth, kernel_size, inner_depth, new_stage=False):
        super().__init__()
        self.new_stage = new_stage
        if (self.new_stage):
            init_strides = 1
            self.shortcut_pad = tf.keras.Sequential([
                tf.keras.layers.Conv1D(filters=input_depth*2, kernel_size=(1,))
                ])
        else:
            init_strides = 1

        self.block_1 = tf.keras.Sequential([
            tf.keras.layers.Conv1D(filters=inner_depth, kernel_size=(1,), strides=init_strides),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Activation('relu'),

            tf.keras.layers.Conv1D(filters=inner_depth, kernel_size=(kernel_size,), padding='same'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Activation('relu'),

            tf.keras.layers.Conv1D(filters=(input_depth/2 if not new_stage else input_depth), kernel_size=(1,)),
            tf.keras.layers.BatchNormalization(),                        
        ])

        self.block_2 = tf.keras.Sequential([
            tf.keras.layers.Conv1D(filters=inner_depth, kernel_size=(1,), strides=init_strides),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Activation('relu'),

            tf.keras.layers.Conv1D(filters=inner_depth, kernel_size=(kernel_size+2,), padding='same'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Activation('relu'),

            tf.keras.layers.Conv1D(filters=(input_depth/2 if not new_stage else input_depth), kernel_size=(1,)),
            tf.keras.layers.BatchNormalization(),                        
        ])

    def call(self, input, *args, **kwargs):
        if (self.new_stage):
            return tf.nn.relu(tf.concat([self.block_1(input),self.block_2(input)], -1) + self.shortcut_pad(input))
        else:
            return tf.nn.relu(tf.concat([self.block_1(input),self.block_2(input)], -1) + input)

conv_model_84_res  = tf.keras.Sequential([
    tf.keras.layers.Conv1D(filters=64,strides=1, kernel_size=(26,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.MaxPool1D(7, strides=1),
    tf.keras.layers.Conv1D(filters=64,
                        kernel_size=(10,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation('relu'),

    ResidualBlock(64, 2, 6),
    ResidualBlock(64, 2, 6),

    tf.keras.layers.AveragePooling1D(44, strides=1),
    tf.keras.layers.Dense(units=1024, activation=tf.keras.layers.Activation('relu'), kernel_regularizer='l2'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(units=512, activation=tf.keras.layers.Activation('relu'), kernel_regularizer='l2'),
    tf.keras.layers.Dense(units=1),
])

conv_model_84 = tf.keras.Sequential([
    tf.keras.layers.Conv1D(filters=64,
                        kernel_size=(26,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation('swish'),
    tf.keras.layers.MaxPool1D(5, strides=1),

    tf.keras.layers.Conv1D(filters=128,
                        kernel_size=(10,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation('swish'),
    tf.keras.layers.AveragePooling1D(46, strides=1),

    tf.keras.layers.Dense(units=512, activation=tf.keras.layers.Activation('swish'), kernel_regularizer='l2'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(units=512, activation=tf.keras.layers.Activation('swish'), kernel_regularizer='l2'),

    tf.keras.layers.Dense(units=1),
])

models = [conv_model_84_res]

MAX_EPOCHS = 60
eval_width = 1000
window_widths = [84]
val_performance = {}
performance = {}

for i, window_width in enumerate(window_widths):
    
    model = models[i]
    conv_width = window_width

    conv_window = WindowGenerator(
        input_width=conv_width,
        label_width=1,
        shift=0,
        strides=500,
        label_columns_indices=[-1])

    wide_eval_window = WindowGenerator(
        input_width=eval_width, 
        label_width=eval_width, 
        shift=0, strides=1, 
        label_columns_indices=[-1])

    wide_conv_window = WindowGenerator(
        input_width=(conv_width-1)+eval_width,
        label_width=eval_width,
        shift=0,
        strides=1,
        label_columns_indices=[-1])

    print("Wide conv window")
    print('Input shape:', wide_conv_window.example[0].shape)
    print('Labels shape:', wide_conv_window.example[1].shape)
    print('Output shape:', model(wide_conv_window.example[0]).shape)

    history = compile_and_fit(model, conv_window)
    model.summary()

    # Save the entire model as a SavedModel.
    !mkdir -p '/content/drive/My Drive/Google Colab/COMP562FinalProject/saved_model'
    tf.keras.models.save_model(model, '/content/drive/My Drive/Google Colab/COMP562FinalProject/saved_model/conv_model_{}'.format(i)) 

    wide_conv_window.plot(model)
    val_performance['Conv'.format(i)] = model.evaluate(conv_window.val)
    performance['Conv'.format(i)] = model.evaluate(conv_window.test, verbose=0)

wide_conv_window = WindowGenerator(
    input_width=(conv_width-1)+5000,
    label_width=5000,
    shift=0,
    strides=1,
    label_columns_indices=[-1])
wide_conv_window.plot(model, max_subplots=1, conf_mat=True)

train_error = history.history['root_mean_squared_error']
val_error = history.history['val_root_mean_squared_error']
plt.figure(figsize=(10,10))
plt.plot(train_error, label='Training RMSE', )
plt.plot(val_error, label='Validation RMSE')
plt.title('Training & Validation RMSE Error')
plt.xlabel('Epoch')
plt.ylabel('RMSE (Degrees)')
plt.legend()

